{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWQp0pChfc3FBV/dhZ+qrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boodie04/Supervised_ML/blob/main/LogisticRegressionScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports"
      ],
      "metadata": {
        "id": "RiQQS8WRn8Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n"
      ],
      "metadata": {
        "id": "WsFm5GUzn-QE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class LogisticRegressionScratch"
      ],
      "metadata": {
        "id": "qO5rWR7en_ua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXWgavF5niQ-",
        "outputId": "9813dbf3-c6f8-49a7-bf72-9f4a59ae1a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Training with GD ==========\n",
            "Epoch 1/50 - Loss: 0.6895\n",
            "Epoch 2/50 - Loss: 0.6859\n",
            "Epoch 3/50 - Loss: 0.6824\n",
            "Epoch 4/50 - Loss: 0.6790\n",
            "Epoch 5/50 - Loss: 0.6756\n",
            "Epoch 6/50 - Loss: 0.6722\n",
            "Epoch 7/50 - Loss: 0.6689\n",
            "Epoch 8/50 - Loss: 0.6656\n",
            "Epoch 9/50 - Loss: 0.6624\n",
            "Epoch 10/50 - Loss: 0.6593\n",
            "Epoch 11/50 - Loss: 0.6562\n",
            "Epoch 12/50 - Loss: 0.6531\n",
            "Epoch 13/50 - Loss: 0.6501\n",
            "Epoch 14/50 - Loss: 0.6472\n",
            "Epoch 15/50 - Loss: 0.6443\n",
            "Epoch 16/50 - Loss: 0.6414\n",
            "Epoch 17/50 - Loss: 0.6386\n",
            "Epoch 18/50 - Loss: 0.6358\n",
            "Epoch 19/50 - Loss: 0.6331\n",
            "Epoch 20/50 - Loss: 0.6304\n",
            "Epoch 21/50 - Loss: 0.6277\n",
            "Epoch 22/50 - Loss: 0.6251\n",
            "Epoch 23/50 - Loss: 0.6225\n",
            "Epoch 24/50 - Loss: 0.6200\n",
            "Epoch 25/50 - Loss: 0.6175\n",
            "Epoch 26/50 - Loss: 0.6150\n",
            "Epoch 27/50 - Loss: 0.6126\n",
            "Epoch 28/50 - Loss: 0.6102\n",
            "Epoch 29/50 - Loss: 0.6079\n",
            "Epoch 30/50 - Loss: 0.6055\n",
            "Epoch 31/50 - Loss: 0.6032\n",
            "Epoch 32/50 - Loss: 0.6010\n",
            "Epoch 33/50 - Loss: 0.5988\n",
            "Epoch 34/50 - Loss: 0.5966\n",
            "Epoch 35/50 - Loss: 0.5944\n",
            "Epoch 36/50 - Loss: 0.5923\n",
            "Epoch 37/50 - Loss: 0.5902\n",
            "Epoch 38/50 - Loss: 0.5881\n",
            "Epoch 39/50 - Loss: 0.5861\n",
            "Epoch 40/50 - Loss: 0.5841\n",
            "Epoch 41/50 - Loss: 0.5821\n",
            "Epoch 42/50 - Loss: 0.5801\n",
            "Epoch 43/50 - Loss: 0.5782\n",
            "Epoch 44/50 - Loss: 0.5763\n",
            "Epoch 45/50 - Loss: 0.5744\n",
            "Epoch 46/50 - Loss: 0.5726\n",
            "Epoch 47/50 - Loss: 0.5708\n",
            "Epoch 48/50 - Loss: 0.5690\n",
            "Epoch 49/50 - Loss: 0.5672\n",
            "Epoch 50/50 - Loss: 0.5654\n",
            "\n",
            "Confusion Matrix:\n",
            "[[91  6]\n",
            " [21 82]]\n",
            "Accuracy : 0.8650\n",
            "Precision: 0.9318\n",
            "Recall   : 0.7961\n",
            "F1 Score : 0.8586\n",
            "\n",
            "========== Training with SGD ==========\n",
            "Epoch 1/50 - Loss: 0.3646\n",
            "Epoch 2/50 - Loss: 0.3541\n",
            "Epoch 3/50 - Loss: 0.3523\n",
            "Epoch 4/50 - Loss: 0.3518\n",
            "Epoch 5/50 - Loss: 0.3517\n",
            "Epoch 6/50 - Loss: 0.3516\n",
            "Epoch 7/50 - Loss: 0.3516\n",
            "Epoch 8/50 - Loss: 0.3516\n",
            "Epoch 9/50 - Loss: 0.3516\n",
            "Epoch 10/50 - Loss: 0.3516\n",
            "Epoch 11/50 - Loss: 0.3516\n",
            "Epoch 12/50 - Loss: 0.3516\n",
            "Epoch 13/50 - Loss: 0.3516\n",
            "Epoch 14/50 - Loss: 0.3516\n",
            "Epoch 15/50 - Loss: 0.3516\n",
            "Epoch 16/50 - Loss: 0.3516\n",
            "Epoch 17/50 - Loss: 0.3516\n",
            "Epoch 18/50 - Loss: 0.3516\n",
            "Epoch 19/50 - Loss: 0.3516\n",
            "Epoch 20/50 - Loss: 0.3516\n",
            "Epoch 21/50 - Loss: 0.3516\n",
            "Epoch 22/50 - Loss: 0.3516\n",
            "Epoch 23/50 - Loss: 0.3516\n",
            "Epoch 24/50 - Loss: 0.3516\n",
            "Epoch 25/50 - Loss: 0.3516\n",
            "Epoch 26/50 - Loss: 0.3516\n",
            "Epoch 27/50 - Loss: 0.3516\n",
            "Epoch 28/50 - Loss: 0.3516\n",
            "Epoch 29/50 - Loss: 0.3516\n",
            "Epoch 30/50 - Loss: 0.3516\n",
            "Epoch 31/50 - Loss: 0.3516\n",
            "Epoch 32/50 - Loss: 0.3516\n",
            "Epoch 33/50 - Loss: 0.3516\n",
            "Epoch 34/50 - Loss: 0.3516\n",
            "Epoch 35/50 - Loss: 0.3516\n",
            "Epoch 36/50 - Loss: 0.3516\n",
            "Epoch 37/50 - Loss: 0.3516\n",
            "Epoch 38/50 - Loss: 0.3516\n",
            "Epoch 39/50 - Loss: 0.3516\n",
            "Epoch 40/50 - Loss: 0.3516\n",
            "Epoch 41/50 - Loss: 0.3516\n",
            "Epoch 42/50 - Loss: 0.3516\n",
            "Epoch 43/50 - Loss: 0.3516\n",
            "Epoch 44/50 - Loss: 0.3516\n",
            "Epoch 45/50 - Loss: 0.3516\n",
            "Epoch 46/50 - Loss: 0.3516\n",
            "Epoch 47/50 - Loss: 0.3516\n",
            "Epoch 48/50 - Loss: 0.3516\n",
            "Epoch 49/50 - Loss: 0.3516\n",
            "Epoch 50/50 - Loss: 0.3516\n",
            "\n",
            "Confusion Matrix:\n",
            "[[90  7]\n",
            " [16 87]]\n",
            "Accuracy : 0.8850\n",
            "Precision: 0.9255\n",
            "Recall   : 0.8447\n",
            "F1 Score : 0.8832\n",
            "\n",
            "========== Training with MBGD ==========\n",
            "Epoch 1/50 - Loss: 0.6174\n",
            "Epoch 2/50 - Loss: 0.5652\n",
            "Epoch 3/50 - Loss: 0.5281\n",
            "Epoch 4/50 - Loss: 0.5004\n",
            "Epoch 5/50 - Loss: 0.4791\n",
            "Epoch 6/50 - Loss: 0.4623\n",
            "Epoch 7/50 - Loss: 0.4488\n",
            "Epoch 8/50 - Loss: 0.4376\n",
            "Epoch 9/50 - Loss: 0.4283\n",
            "Epoch 10/50 - Loss: 0.4204\n",
            "Epoch 11/50 - Loss: 0.4136\n",
            "Epoch 12/50 - Loss: 0.4078\n",
            "Epoch 13/50 - Loss: 0.4027\n",
            "Epoch 14/50 - Loss: 0.3982\n",
            "Epoch 15/50 - Loss: 0.3943\n",
            "Epoch 16/50 - Loss: 0.3908\n",
            "Epoch 17/50 - Loss: 0.3877\n",
            "Epoch 18/50 - Loss: 0.3849\n",
            "Epoch 19/50 - Loss: 0.3824\n",
            "Epoch 20/50 - Loss: 0.3802\n",
            "Epoch 21/50 - Loss: 0.3781\n",
            "Epoch 22/50 - Loss: 0.3762\n",
            "Epoch 23/50 - Loss: 0.3745\n",
            "Epoch 24/50 - Loss: 0.3730\n",
            "Epoch 25/50 - Loss: 0.3716\n",
            "Epoch 26/50 - Loss: 0.3703\n",
            "Epoch 27/50 - Loss: 0.3691\n",
            "Epoch 28/50 - Loss: 0.3680\n",
            "Epoch 29/50 - Loss: 0.3670\n",
            "Epoch 30/50 - Loss: 0.3660\n",
            "Epoch 31/50 - Loss: 0.3651\n",
            "Epoch 32/50 - Loss: 0.3643\n",
            "Epoch 33/50 - Loss: 0.3636\n",
            "Epoch 34/50 - Loss: 0.3629\n",
            "Epoch 35/50 - Loss: 0.3622\n",
            "Epoch 36/50 - Loss: 0.3616\n",
            "Epoch 37/50 - Loss: 0.3610\n",
            "Epoch 38/50 - Loss: 0.3605\n",
            "Epoch 39/50 - Loss: 0.3600\n",
            "Epoch 40/50 - Loss: 0.3596\n",
            "Epoch 41/50 - Loss: 0.3591\n",
            "Epoch 42/50 - Loss: 0.3587\n",
            "Epoch 43/50 - Loss: 0.3583\n",
            "Epoch 44/50 - Loss: 0.3580\n",
            "Epoch 45/50 - Loss: 0.3576\n",
            "Epoch 46/50 - Loss: 0.3573\n",
            "Epoch 47/50 - Loss: 0.3570\n",
            "Epoch 48/50 - Loss: 0.3567\n",
            "Epoch 49/50 - Loss: 0.3565\n",
            "Epoch 50/50 - Loss: 0.3562\n",
            "\n",
            "Confusion Matrix:\n",
            "[[90  7]\n",
            " [18 85]]\n",
            "Accuracy : 0.8750\n",
            "Precision: 0.9239\n",
            "Recall   : 0.8252\n",
            "F1 Score : 0.8718\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Logistic Regression Class Implementation\n",
        "class LogisticRegressionScratch:\n",
        "\n",
        "    # Constructor to initialize hyperparameters and other variables\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000, optimizer='gd', batch_size=32):\n",
        "        self.learning_rate = learning_rate  # Learning rate for gradient descent\n",
        "        self.epochs = epochs  # Number of training iterations (epochs)\n",
        "        self.optimizer = optimizer  # Type of optimizer: 'gd', 'sgd', 'mbgd'\n",
        "        self.batch_size = batch_size  # Size of the mini-batches for MBGD\n",
        "        self.losses = []  # To store the loss value at each epoch for monitoring training progress\n",
        "\n",
        "    # Sigmoid function to transform the linear output to a probability\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))  # Sigmoid function, which outputs a value between 0 and 1\n",
        "\n",
        "    # Initialize weights (W) to zeros and bias (b) to zero\n",
        "    def initialize_weights(self, n_features):\n",
        "        self.W = np.zeros(n_features)  # Initialize weights to zero, one for each feature\n",
        "        self.b = 0.  # Initialize bias to zero\n",
        "\n",
        "    # Compute the loss using binary cross-entropy (logistic loss function)\n",
        "    def compute_loss(self, y, y_pred):\n",
        "        # Clip predicted values to prevent log(0) error (because log(0) is undefined)\n",
        "        epsilon = 1e-15  # Small value to avoid log(0)\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip the predicted values\n",
        "        # Binary cross-entropy loss formula\n",
        "        return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))  # Average loss over all samples\n",
        "\n",
        "    # Predict probabilities using the sigmoid function\n",
        "    def predict_proba(self, X):\n",
        "        return self.sigmoid(np.dot(X, self.W) + self.b)  # Linear combination of inputs + bias, passed through sigmoid\n",
        "\n",
        "    # Predict the class labels (0 or 1) based on the probability threshold of 0.5\n",
        "    def predict(self, X):\n",
        "        return (self.predict_proba(X) >= 0.5).astype(int)  # If probability >= 0.5, classify as 1, else 0\n",
        "\n",
        "    # Update weights using the gradient descent method\n",
        "    def update_weights(self, X, y):\n",
        "        y_pred = self.predict_proba(X)  # Get the predicted probabilities\n",
        "        error = y_pred - y  # Calculate the error (difference between predicted and true labels)\n",
        "        # Compute gradients (derivatives of the loss w.r.t. weights and bias)\n",
        "        dw = np.dot(X.T, error) / len(y)  # Gradient w.r.t. weights (X.T is the transpose of X)\n",
        "        db = np.sum(error) / len(y)  # Gradient w.r.t. bias\n",
        "        # Update weights and bias using the gradients and learning rate\n",
        "        self.W -= self.learning_rate * dw  # Update weights\n",
        "        self.b -= self.learning_rate * db  # Update bias\n",
        "\n",
        "    # Training function to fit the model to the data using different optimizers\n",
        "    def fit(self, X, y):\n",
        "        self.initialize_weights(X.shape[1])  # Initialize weights based on the number of features\n",
        "\n",
        "        # Iterate through each epoch (training cycle)\n",
        "        for epoch in range(self.epochs):\n",
        "            # Gradient Descent (GD)\n",
        "            if self.optimizer == 'gd':\n",
        "                self.update_weights(X, y)\n",
        "\n",
        "            # Stochastic Gradient Descent (SGD) - one sample at a time\n",
        "            elif self.optimizer == 'sgd':\n",
        "                for i in range(len(y)):\n",
        "                    self.update_weights(X[i:i+1], y[i:i+1])  # Update weights for each individual sample\n",
        "\n",
        "            # Mini-Batch Gradient Descent (MBGD) - using small batches of data\n",
        "            elif self.optimizer == 'mbgd':\n",
        "                indices = np.arange(len(y))  # Create an array of indices from 0 to len(y)-1\n",
        "                np.random.shuffle(indices)  # Shuffle indices to introduce randomness\n",
        "                # Iterate over the shuffled indices in mini-batches\n",
        "                for start in range(0, len(y), self.batch_size):\n",
        "                    end = start + self.batch_size\n",
        "                    batch_indices = indices[start:end]  # Select the batch of data\n",
        "                    self.update_weights(X[batch_indices], y[batch_indices])  # Update weights using the mini-batch\n",
        "\n",
        "            # Compute the loss after each epoch and store it\n",
        "            y_pred = self.predict_proba(X)  # Get predicted probabilities\n",
        "            loss = self.compute_loss(y, y_pred)  # Compute the loss\n",
        "            self.losses.append(loss)  # Append loss to the list for monitoring\n",
        "            # Print the current epoch and loss for tracking\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs} - Loss: {loss:.4f}\")\n",
        "\n",
        "    # Evaluation function to calculate various performance metrics\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred = self.predict(X)  # Get predicted class labels (0 or 1)\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_true, y_pred))  # Print confusion matrix\n",
        "        # Print other classification metrics\n",
        "        print(f\"Accuracy : {accuracy_score(y_true, y_pred):.4f}\")\n",
        "        print(f\"Precision: {precision_score(y_true, y_pred):.4f}\")\n",
        "        print(f\"Recall   : {recall_score(y_true, y_pred):.4f}\")\n",
        "        print(f\"F1 Score : {f1_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "# ========== Run Logistic Regression with Comparison ==========\n",
        "\n",
        "# Generate synthetic binary classification data (X: features, y: labels)\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=42)\n",
        "\n",
        "# Split data into training and test sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of optimizers to test\n",
        "optimizers = ['gd', 'sgd', 'mbgd']\n",
        "\n",
        "# Train and evaluate the model using each optimizer\n",
        "for opt in optimizers:\n",
        "    print(f\"\\n========== Training with {opt.upper()} ==========\")\n",
        "    model = LogisticRegressionScratch(learning_rate=0.01, epochs=50, optimizer=opt, batch_size=32)\n",
        "    model.fit(X_train, y_train)  # Train the model\n",
        "    model.evaluate(X_test, y_test)  # Evaluate the model on the test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ The Binary Cross-Entropy Loss Function\n",
        "This is the line weâ€™re focusing on:\n",
        "\n",
        "return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
        "This loss function calculates the error between the actual labels y and the predicted probabilities y_pred. We want to minimize this error during training.\n",
        "\n",
        "ðŸ§  How It Works:\n",
        "Element-wise Operations: The terms y * np.log(y_pred) and (1 - y) * np.log(1 - y_pred) are computed for each data point. These are the individual losses for each example.\n",
        "What do these parts do?\n",
        "\n",
        "y * np.log(y_pred):\n",
        "When y = 1, this term becomes the log loss for when the model predicts a high probability of 1.\n",
        "If y_pred is close to 1, the log term becomes very small (low loss). If y_pred is close to 0, the log term becomes large (high loss).\n",
        "(1 - y) * np.log(1 - y_pred):\n",
        "When y = 0, this term becomes the log loss for when the model predicts a low probability of 1 (high probability for class 0).\n",
        "If y_pred is close to 0, the log term is small (low loss). If y_pred is close to 1, the log term becomes large (high loss).\n",
        "Summing All the Losses: After the element-wise operations, you get an array of individual losses for each data point.\n",
        "Example:\n",
        "\n",
        "y = [1, 0, 1]\n",
        "y_pred = [0.9, 0.2, 0.8]\n",
        "\n",
        "loss_1 = 1 * np.log(0.9)  # For the first example\n",
        "loss_2 = 0 * np.log(0.2) + 1 * np.log(0.8)  # For the second example\n",
        "loss_3 = 1 * np.log(0.8)  # For the third example\n",
        "After calculating each loss, the result would look like this:\n",
        "\n",
        "losses = [loss_1, loss_2, loss_3]\n",
        "Computing the Average Loss:\n",
        "np.mean() calculates the average of these individual losses across all samples.\n",
        "This gives the final average loss for the entire dataset.\n",
        "If we had, for example:\n",
        "\n",
        "losses = [0.1054, 0.2231, 0.1405]  # Example losses\n",
        "np.mean(losses)\n",
        "This would compute:\n",
        "\n",
        "0.1054\n",
        "+\n",
        "0.2231\n",
        "+\n",
        "0.1405\n",
        "3\n",
        "=\n",
        "0.1563\n",
        "â€˜\n",
        "â€˜\n",
        "â€˜\n",
        "3\n",
        "0.1054+0.2231+0.1405\n",
        "â€‹\n",
        " =0.1563â€˜â€˜â€˜\n",
        "Negative Sign: The minus sign - in front of np.mean() is there because we want the loss to be positive. The log terms give negative values, so we negate it to make the loss positive.\n"
      ],
      "metadata": {
        "id": "_gOAlH8ytxIN"
      }
    }
  ]
}